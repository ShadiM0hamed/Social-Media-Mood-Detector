{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YY1ZhfgTDpv",
        "outputId": "3757abb3-3dde-4c8d-af7e-72f1ea1c549e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 24.7 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 49.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.1 contractions-0.1.72 pyahocorasick-1.4.4 textsearch-0.0.24\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "nltk.download('omw-1.4')\n",
        "!pip install contractions\n",
        "import contractions\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "moods = {0:'sadness',\n",
        "         1:'anger',\n",
        "         2:'love',\n",
        "         3:'surprise',\n",
        "         4:'fear',\n",
        "         5:'joy'}\n",
        "\n",
        "def Lemm(sentence):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    sentence = contractions.fix(sentence)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    sentence = re.sub('[^A-z]', ' ', sentence)\n",
        "    negative = ['not', 'neither', 'nor', 'but', 'however',\n",
        "                'although', 'nonetheless', 'despite', 'except',\n",
        "                        'even though', 'yet','unless']\n",
        "    stop_words = [z for z in stop_words if z not in negative]\n",
        "    preprocessed_tokens = [lemmatizer.lemmatize(contractions.fix(temp.lower())) for temp in sentence.split() if temp not in stop_words]\n",
        "    return ' '.join([x for x in preprocessed_tokens]).strip()\n",
        "    \n",
        "vectorizer = pickle.load(open(\"/content/drive/MyDrive/Colab Notebooks/archive/vector.pickel\", \"rb\"))\n",
        "LR = pickle.load(open(\"/content/drive/MyDrive/Colab Notebooks/archive/LRModel.sav\", \"rb\"))"
      ],
      "metadata": {
        "id": "lEylcHwzTQXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "moods[LR.predict(vectorizer.transform([Lemm('Shady Don\\'t know what to do')]))[0]].capitalize() + ' :('"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CoWVe5vwUARK",
        "outputId": "9684217e-16a6-4154-fcea-7de215547d4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sadness :('"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-wKtERk3exge"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}